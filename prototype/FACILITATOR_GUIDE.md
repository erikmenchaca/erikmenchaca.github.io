# Facilitator Guide - Cooply Usability Testing üéØ

## Pre-Session Checklist

### 30 Minutes Before Participant Arrives:

#### Technical Setup
- [ ] Open prototype in browser (full screen mode recommended)
- [ ] Test that all pages load correctly
- [ ] Clear any previous session data (refresh browser)
- [ ] Set up screen recording software
- [ ] Test audio recording
- [ ] Have backup recording method ready

#### Materials Ready
- [ ] **PARTICIPANT_INSTRUCTIONS.md** printed or ready to share
- [ ] Task scenario cards (see below)
- [ ] Emoji rating sheets (printed)
- [ ] 1-10 rating cards
- [ ] SUS questionnaire printed
- [ ] Custom research questions printed
- [ ] Consent form (if required)
- [ ] Pen/pencil for participant

#### Workspace Setup
- [ ] Comfortable seating
- [ ] Water for participant
- [ ] Minimize distractions
- [ ] "Testing in Progress" sign on door

---

## Session Structure (45 minutes total)

### 1. Introduction (5 minutes)

#### Welcome Script:
> "Hi [Name], thank you for coming! I'm [Your Name], and today you'll be testing Cooply, an AI-powered job search platform. 
>
> **Important things to know:**
> - We're testing the design, not you
> - There are no right or wrong answers
> - Please think aloud as you work
> - Feel free to ask questions anytime
> - Everything you say is helpful
>
> I'll ask you to complete some tasks, then we'll discuss your experience. The whole session takes about 45 minutes.
>
> Before we start, I'll begin recording the screen and audio. Is that okay?"

#### Get Consent
- [ ] Explain recording purpose
- [ ] Get verbal/written consent
- [ ] Start recording

#### Participant Background (2 min)
Ask quickly:
- "Have you job searched in the past year?"
- "Have you used any AI tools before?"
- "On a scale of 1-10, how confident do you feel about job searching?" *[Record baseline]*

---

### 2. Introduction to Prototype (3 minutes)

#### Show Interface
> "This is Cooply. Let me show you the basics:"

Point out:
- **Sidebar navigation** - "Here's your main menu"
- **AI agents section** - "These are 4 specialized AI helpers"
- **Main workspace** - "This is where you'll see content"

Give participant instructions:
- [ ] Hand them **PARTICIPANT_INSTRUCTIONS.md** OR explain verbally
- [ ] Set the scenario: "For this test, imagine you're Alex Chen, a UX Designer with 3 years experience, looking for Senior UX roles in San Francisco."

#### Start Think-Aloud
> "As you work, please say what you're thinking out loud. For example: 'I'm looking for the save button' or 'I wonder what this does.' Your thoughts help us understand your experience."

---

### 3. Tasks (25-30 minutes)

**For each task:**
1. Read scenario card
2. Let participant work independently
3. Observe silently (take notes)
4. If stuck >2 minutes, ask: "What are you thinking?"
5. Give hint only if completely blocked
6. After task, ask follow-up questions

---

## TASK 1: Job Discovery & Understanding Matches
**Time: 5-7 minutes**

### Scenario Card:
> "You're looking for UX Design jobs in San Francisco. Find 3 jobs that interest you and save them for later."

### What to Observe:
- [ ] Do they find the Job Search page?
- [ ] Do they notice match scores?
- [ ] Do they read "Why this match?" explanations?
- [ ] Can they save jobs?
- [ ] Do they comment on time-saved indicator?

### After Task - Ask:
1. "How confident are you that these jobs match your skills?" (1-10)
2. "Did you notice anything that explained why jobs were recommended?"
3. "Did you feel this was faster than job sites you've used before?" (Why/why not?)

### Research Questions Tested:
- ‚úÖ **Q1**: Speed (time taken, self-reported)
- ‚úÖ **Q4**: Trust (confidence in matches, noticed explanations)

---

## TASK 2: Understanding AI Agents
**Time: 5 minutes**

### Scenario Card:
> "You have an interview coming up next week. Use Cooply to get help preparing for it."

### What to Observe:
- [ ] Do they identify the Interview Coach agent?
- [ ] Do they understand the sidebar descriptions?
- [ ] Do they use tooltips?
- [ ] Can they navigate to the right agent?

### After Task - Ask:
1. "Before I asked you to do this, could you tell what each AI agent does just by looking at the sidebar?" 
2. "Can you explain what the Smart Match agent does?" *[Test comprehension]*
3. "What about the Application Tracker?" *[Test comprehension]*
4. "Were the descriptions clear?" (Yes/No, explain)

### Bonus Question:
Show agent icon/color, ask: "Which agent is this?"

### Research Questions Tested:
- ‚úÖ **Q2**: Understanding (can identify agents, comprehension accuracy)

---

## TASK 3: Navigation & Tracking
**Time: 5 minutes**

### Scenario Card:
> "Check the status of your Google application and see what you need to do next."

### What to Observe:
- [ ] Can they find Applications page without help?
- [ ] Do they notice the success celebration banner?
- [ ] Do they understand status badges?
- [ ] Do they notice AI insights/recommendations?

### After Task - Ask:
1. "Was it easy to find where to track your applications?" (1-10)
2. "Did you always know where you were in the app?" (Yes/No)
3. "Did you notice any suggestions or insights?" (What were they?)

### Follow-up (show Applications page):
> "How does seeing this [success banner] make you feel?"

### Research Questions Tested:
- ‚úÖ **Q3**: Navigation (ease, awareness of location)
- ‚úÖ **Q5**: Emotion (reaction to success banner, confidence)

---

## TASK 4: AI Chat Interaction
**Time: 5-7 minutes**

### Scenario Card:
> "You're wondering if there are more UX jobs available in other cities. Ask the AI for help."

### What to Observe:
- [ ] Do they find the AI chat button?
- [ ] Do they understand how to interact?
- [ ] Do they read AI responses?
- [ ] Do they trust the information given?

### After Task - Ask:
1. "Did the AI's response feel helpful?" (1-10)
2. "Did you trust the information it gave you?" (Why/why not?)
3. "How does this compare to using a regular job search site?"

### Research Questions Tested:
- ‚úÖ **Q4**: Trust (AI credibility, specific data points)
- ‚úÖ **Q1**: Speed (faster than manual search?)

---

## TASK 5: Free Exploration
**Time: 3-5 minutes**

### Scenario Card:
> "Explore Cooply on your own. Click around and try anything that interests you."

### What to Observe:
- [ ] What do they gravitate toward?
- [ ] Do they discover features you didn't show?
- [ ] What confuses them?
- [ ] What delights them?

### After Task - Ask:
1. "What was your favorite feature?"
2. "Was anything confusing or frustrating?"
3. "Is there anything you expected to see but didn't?"

### Research Questions Tested:
- All (general usability, discoverability)

---

## 4. Post-Task Discussion (5-7 minutes)

### Emotional State Assessment

#### Show Emoji Scale:
```
üòü Confused    üòê Neutral    üòä Confident    üòÑ Very Confident
```

**Ask:** "Point to how you feel about using Cooply for job searching."

*[Record response for Q5]*

---

### Open-Ended Questions:

1. **Overall Impression**
   > "In one sentence, how would you describe Cooply?"

2. **AI Value**
   > "Did the AI agents feel helpful or gimmicky?" (Explain)

3. **Comparison**
   > "How does this compare to Indeed, LinkedIn, or other job sites you've used?"

4. **Trust Deep Dive**
   > "Would you trust Cooply's job recommendations? Why or why not?"

5. **Speed Perception**
   > "Do you think this would help you find jobs faster?" (Explain)

6. **What's Missing**
   > "Is there anything you wish Cooply had?"

7. **Would You Use It?**
   > "If this were a real product, would you use it?" (Why/why not?)

---

## 5. Questionnaires (5-7 minutes)

### System Usability Scale (SUS)
Hand participant the **SUS questionnaire** (10 questions, 1-5 scale)

> "Please rate your agreement with each statement."

*[Standard SUS scoring: Odd items score (rating - 1), even items score (5 - rating), sum and multiply by 2.5]*

---

### Custom Research Questions

#### Q1: Speed & Efficiency
1. "Compared to traditional job sites, Cooply helps me find jobs:"
   ```
   Much Slower  1  2  3  4  5  6  7  Much Faster
   ```

2. "The AI saved me time during job search:"
   ```
   Strongly Disagree  1  2  3  4  5  Strongly Agree
   ```

#### Q2: Agent Understanding
3. "I understand what each AI agent does:"
   ```
   Strongly Disagree  1  2  3  4  5  Strongly Agree
   ```

4. "The agent names and descriptions are clear:"
   ```
   Strongly Disagree  1  2  3  4  5  Strongly Agree
   ```

#### Q3: Navigation Ease
5. "The layout is easy to navigate:"
   ```
   Strongly Disagree  1  2  3  4  5  Strongly Agree
   ```

6. "I always knew where I was in the app:"
   ```
   Strongly Disagree  1  2  3  4  5  Strongly Agree
   ```

#### Q4: Trust in AI
7. "I trust the AI's job recommendations:"
   ```
   Strongly Disagree  1  2  3  4  5  Strongly Agree
   ```

8. "The 'Why this match?' explanations are helpful:"
   ```
   Strongly Disagree  1  2  3  4  5  Strongly Agree
   ```

9. "Match scores accurately reflect job fit:"
   ```
   Strongly Disagree  1  2  3  4  5  Strongly Agree
   ```

#### Q5: Emotional Response
10. "After using Cooply, I feel:"
    ```
    ‚óã Confused    ‚óã Neutral    ‚óã Confident    ‚óã Very Confident
    ```

11. "Using AI assistance made me feel:"
    ```
    More Anxious  1  2  3  4  5  6  7  More Confident
    ```

12. "I know what my next steps are:"
    ```
    Strongly Disagree  1  2  3  4  5  Strongly Agree
    ```

---

## 6. Wrap-Up (2 minutes)

### Thank Participant
> "Thank you so much for your time and honest feedback! Your insights will directly improve Cooply and help future job seekers."

### Compensation (if applicable)
- [ ] Provide gift card/incentive
- [ ] Get receipt signed

### Ask Permission
> "May we contact you for follow-up questions if needed?"
- [ ] Get email (if yes)

### Stop Recording
- [ ] End screen recording
- [ ] End audio recording
- [ ] Save files immediately

---

## Post-Session Checklist

### Immediately After Participant Leaves:

#### Debrief Notes (5 min)
While memory is fresh, write:
- **Most surprising observation?**
- **Biggest pain point?**
- **Most successful moment?**
- **Quote to remember:**
- **Design changes to consider:**

#### Data Organization (5 min)
- [ ] Save recordings with participant ID (not name)
- [ ] Scan/digitize paper questionnaires
- [ ] File consent forms securely
- [ ] Back up all files

#### Prepare for Next Session (5 min)
- [ ] Refresh browser (clear previous session)
- [ ] Reprint any used materials
- [ ] Review notes for patterns
- [ ] Adjust script if needed (only minor tweaks)

---

## Observation Cheat Sheet

### What to Watch For:

#### Body Language
- **Leaning forward** = engaged, interested
- **Leaning back** = confused, overwhelmed
- **Frowning/squinting** = struggling to understand
- **Smiling** = pleased, delighted
- **Sighing** = frustrated

#### Mouse Behavior
- **Hovering without clicking** = uncertainty
- **Rapid clicking** = frustration or urgency
- **Slow, deliberate clicks** = careful, thoughtful
- **Scrolling extensively** = searching for something

#### Verbal Cues
- **"Hmm..."** = thinking, processing
- **"I wonder..."** = exploring, curious
- **"Where is..."** = lost, navigation issue
- **"Oh!"** = discovery, surprise (good or bad)
- **"I expected..."** = mental model mismatch

---

## Common Facilitator Mistakes to Avoid

### ‚ùå Don't Do This:
1. **Leading questions**: "That was easy, right?" ‚ùå
   - **Better**: "How was that?" ‚úÖ

2. **Helping too quickly**: Immediately showing them
   - **Better**: Wait 2 minutes, then ask "What are you thinking?" ‚úÖ

3. **Defending the design**: "Well, it's supposed to..."
   - **Better**: "That's interesting, tell me more" ‚úÖ

4. **Skipping think-aloud**: Letting them work silently
   - **Better**: "Remember to say what you're thinking" ‚úÖ

5. **Rushing through tasks**: "Okay, next task!"
   - **Better**: Ask follow-up questions first ‚úÖ

---

## Data Recording Template

### Participant ID: _______
### Date: _______
### Session Time: Start _______ End _______

#### Demographics:
- Job search experience: _______________
- AI familiarity: _______________
- Baseline confidence (1-10): _______

#### Task Performance:

| Task | Completion | Time | Errors | Notes |
|------|------------|------|--------|-------|
| 1. Job Discovery | ‚óã Yes ‚óã No ‚óã With help | ___ min | ___ | |
| 2. Agent Understanding | ‚óã Yes ‚óã No ‚óã With help | ___ min | ___ | |
| 3. Navigation | ‚óã Yes ‚óã No ‚óã With help | ___ min | ___ | |
| 4. AI Chat | ‚óã Yes ‚óã No ‚óã With help | ___ min | ___ | |
| 5. Free Exploration | - | ___ min | - | |

#### Research Question Metrics:

**Q1: Speed**
- Time to find 3 jobs: _______ min
- Noticed time-saved indicator: ‚óã Yes ‚óã No
- Perceived faster than traditional: ‚óã Yes ‚óã No ‚óã Unsure

**Q2: Understanding**
- Can explain Smart Match: ‚óã Yes ‚óã No ‚óã Partial
- Can explain App Tracker: ‚óã Yes ‚óã No ‚óã Partial
- Can explain Interview Coach: ‚óã Yes ‚óã No ‚óã Partial
- Can explain Insights: ‚óã Yes ‚óã No ‚óã Partial
- Accuracy: ____%

**Q3: Navigation**
- SUS Score: _______
- Task completion without help: ‚óã 100% ‚óã 75% ‚óã 50% ‚óã <50%
- Critical errors: _______

**Q4: Trust**
- Trust rating (1-10): _______
- Read "Why this match?": ‚óã Yes ‚óã No
- Believes AI saves time: ‚óã Yes ‚óã No ‚óã Unsure

**Q5: Emotion**
- Post-task emotion: ‚óã Confused ‚óã Neutral ‚óã Confident ‚óã Very Confident
- Confidence change: Baseline _____ ‚Üí Post-test _____
- Noticed success banner: ‚óã Yes ‚óã No

#### Memorable Quotes:
1. "_________________________________________"
2. "_________________________________________"
3. "_________________________________________"

#### Issues Found:
1. _________________________________________
2. _________________________________________
3. _________________________________________

#### Positive Reactions:
1. _________________________________________
2. _________________________________________
3. _________________________________________

---

## Success Criteria Tracking

After each session, check off if participant met target:

### Q1: Speed
- [ ] Found 3 jobs in < 5 minutes
- [ ] Noticed time-saved indicator
- [ ] Rated faster than traditional (5+ on 7-point scale)

### Q2: Understanding
- [ ] Explained all 4 agents correctly (90%+ accuracy)
- [ ] Noticed sidebar descriptions
- [ ] Could match agents to tasks

### Q3: Navigation
- [ ] Completed all tasks without help
- [ ] SUS score > 75
- [ ] Zero critical navigation errors

### Q4: Trust
- [ ] Trust rating 7+/10
- [ ] Read "Why this match?" explanation
- [ ] Would follow AI suggestion

### Q5: Emotion
- [ ] Reported Confident or Very Confident
- [ ] Confidence increased from baseline
- [ ] Noticed success elements

---

## After 5 Participants

### Pattern Analysis
Look for:
- **Common pain points** (mentioned by 3+ people)
- **Consistent successes** (worked well for 4+ people)
- **Surprising discoveries** (unexpected behavior)
- **Quotes that represent user sentiment**

### Prepare Report
Use data to answer research questions:
- Q1: Did AI make job search faster? (Evidence: _______)
- Q2: Did users understand agents? (Evidence: _______)
- Q3: Was navigation clear? (Evidence: _______)
- Q4: Did users trust AI? (Evidence: _______)
- Q5: How did users feel? (Evidence: _______)

---

## Emergency Scenarios

### If Prototype Crashes:
1. Stay calm: "No problem, this is why we test!"
2. Refresh browser
3. If persists: "Let's continue with talking through what you'd expect to happen"
4. Note the bug for fixing

### If Participant is Very Nervous:
1. Reassure: "You're doing great, this is really helpful"
2. Emphasize: "We're testing the design, there's no right answer"
3. Slow down, give more time

### If Participant Finishes Too Quickly (<20 min):
1. Ask deeper questions: "Tell me more about..."
2. Give additional scenario: "If you were actually job searching, what would you do next?"

### If Running Over Time:
1. Prioritize: Complete current task + questionnaires
2. Skip free exploration if needed
3. Keep wrap-up brief
4. Offer to stay (optional)

---

## You're Ready! üéØ

- ‚úÖ Setup checklist complete
- ‚úÖ Script prepared
- ‚úÖ Tasks ready
- ‚úÖ Recording equipment tested
- ‚úÖ Questionnaires printed
- ‚úÖ Data recording template ready

**Good luck with your testing sessions!** 

Remember: Every participant's feedback is valuable, even if they struggle. That's the point!

---

*Facilitator Guide v1.0 | Last Updated: October 25, 2024*
